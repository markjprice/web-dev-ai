> **Prompt**: Please show me some examples where adding an index makes a query slower, not faster.

It feels counterintuitive—like buying a faster car only to get to work later—but indexes are not "free" performance. They are physical data structures that require maintenance, memory, and disk I/O.

There are three primary ways an index can actually slow you down: **Write Overhead**, **Optimizer Confusion**, and **The Tipping Point**.

---

## 1. Write Overhead (The "Maintenance Tax")

Every time you `INSERT`, `UPDATE`, or `DELETE` a row, the database doesn't just change the table; it must also update every single index associated with that table.

* **The Problem:** If you have a table with 10 indexes, a single `INSERT` statement triggers 11 write operations.
* **Real-World Impact:** In high-volume logging or "ingestion" systems, adding an index can cause the ingestion rate to plummet.
* **The "Update" Penalty:** If you update a column that is indexed, the database has to delete the old entry in the B-Tree and "re-sort" the new entry into its proper location.

---

## 2. Low Selectivity (The "Tipping Point")

An index is like the index at the back of a textbook. It’s great if you’re looking for a specific word like "Photosynthesis." But what if you’re looking for the word "The"?

* **The Scenario:** You index the `status` column of an `Orders` table where 90% of rows are `shipped`.
* **Why it's slower:**
1. The engine looks at the index to find "shipped."
2. It finds 900,000 matches.
3. For *each* match, it has to do a "Bookmark Lookup" to find the actual row on the disk.


* **The Result:** This causes massive **Random I/O**. It is significantly faster for the database to just perform a **Sequential Table Scan**, reading the file once from start to finish, rather than bouncing back and forth 900,000 times between the index and the table.

---

## 3. The "Over-Indexing" Optimizer Trap

The Query Optimizer has a limited "time budget" to decide how to run your query. If you provide it with too many choices, it can make a "Bad Plan" or spend too much time just thinking.

* **Index Merging:** Sometimes, the engine sees two indexes and decides to use both, performing an "Index Merge." This requires reading both indexes and calculating their intersection in memory. Often, just using one index (or a single composite index) would have been faster.
* **The "Index Scan" instead of "Table Scan":** Sometimes a developer creates a "Covering Index" that is very wide (lots of columns). If that index becomes nearly as large as the original table, the engine might choose an **Index Scan**. Because indexes are often more fragmented than tables, reading the whole index can actually take more I/O than reading the whole table.

---

## 4. The "Small Table" Penalty

If your table is small enough to fit into a single block of memory (usually 8KB, or about 100-200 rows), an index is a pure liability.

* **The Logic:** The database has to load the index from disk, search the B-Tree, and then load the table. It’s much faster to just load the table and scan the 100 rows in microseconds. Using an index here adds "overhead" for no gain.

---

## Summary: When to be Careful

| Scenario | Impact of Index | Recommended Action |
| --- | --- | --- |
| **High-Volume Inserts** | Slower writes | Keep indexes minimal. |
| **Boolean/Gender Columns** | Usually ignored or slower | Use "Filtered/Partial Indexes" if available. |
| **Frequently Updated Columns** | High CPU/IO cost | Avoid indexing columns that change every second. |
| **Small "Config" Tables** | Slower reads (marginally) | Do not index tables with < 500 rows. |

### The "Partial Index" Solution

If you are using **PostgreSQL**, you can solve the "Low Selectivity" problem by indexing only the data you care about:

```sql
-- Only indexes the 1% of rows that actually need attention
CREATE INDEX idx_pending_orders ON orders (created_at) 
WHERE status = 'pending';

```
