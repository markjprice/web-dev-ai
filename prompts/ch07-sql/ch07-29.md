> **Prompt**: Please show me how to debug a slow SQL query step by step.

Debugging a slow query is a process of elimination. You are essentially acting as a detective, moving from high-level symptoms down to the specific disk blocks causing the delay.

Here is the professional workflow for diagnosing and fixing a slow SQL query.

---

## Step 1: Confirm the "Slow" and Check the Environment

Before touching the code, determine if the problem is the **query** or the **server**.

* **Check Server Health:** Is the CPU pegged at 100%? Is the disk I/O saturated? A perfectly optimized query will run slowly if the server is out of memory.
* **Isolate the Query:** Run the query in a standalone console. If it’s slow there, it’s a database issue. If it’s fast there but slow in your app, the bottleneck is likely network latency or your application’s ORM mapping logic.

---

## Step 2: Use `EXPLAIN` to Find the "Bad Path"

Run the query prefixed with `EXPLAIN` (or `EXPLAIN ANALYZE` in a safe environment like Staging).

**Look for these "Red Flags" in the output:**

* **Sequential Scans (Seq Scan):** This means the engine is reading the entire table. For large tables, this is almost always the cause of slowness.
* **Nested Loops on Large Sets:** If you are joining two tables with millions of rows and see a "Nested Loop," it means the engine is looping millions of times. A "Hash Join" or "Merge Join" would likely be faster.
* **Cost Disparity:** Look for the step where the "Cost" or "Actual Time" jumps significantly. That is your bottleneck.

---

## Step 3: Check for "Sargability" (Search Arguments)

"Sargable" queries are those that can actually use an index. Many developers accidentally "break" their indexes with bad syntax.

* **Avoid functions on columns:** * ❌ `WHERE YEAR(created_at) = 2026` (Forces a table scan)
* ✅ `WHERE created_at >= '2026-01-01' AND created_at < '2027-01-01'` (Uses index)


* **Avoid leading wildcards:**
* ❌ `WHERE name LIKE '%Smith'` (Forces table scan)
* ✅ `WHERE name LIKE 'Smith%'` (Uses index)



---

## Step 4: Evaluate Indexing Strategy

If the `EXPLAIN` plan shows a Sequential Scan, you likely need an index. But don't just add one blindly.

* **Index Selectivity:** Ensure you aren't indexing a column with low selectivity (like a Boolean `is_active`).
* **Composite Indexes:** If your `WHERE` clause uses two columns (e.g., `WHERE org_id = 5 AND status = 'active'`), a single composite index on `(org_id, status)` is much faster than two separate indexes.
* **Covering Indexes:** If you only need two columns, include them both in the index so the engine never has to visit the main table at all.

---

## Step 5: Check Table Statistics and Bloat

Sometimes the plan is bad because the database has "forgotten" what the data looks like.

* **Update Statistics:** Run `ANALYZE table_name` (PostgreSQL/MySQL). This refreshes the internal metadata so the optimizer can make better choices.
* **Check for Bloat:** In PostgreSQL, if you do many updates/deletes, the table can become "bloated" with dead rows. Run `VACUUM` to clean up.

---

## Step 6: Refactor the Logic

If indexes don't help, the query itself might be fundamentally inefficient.

* **Avoid `SELECT *`:** Only pull the columns you need. This reduces memory usage and disk I/O.
* **Replace Subqueries with Joins:** Modern optimizers are usually better at optimizing `JOIN` logic than `WHERE IN (SELECT ...)` subqueries.
* **Use Pagination Properly:** Never pull 100,000 rows to the front end. Use `LIMIT` and `OFFSET` (or better yet, keyset pagination).

---

## Summary Checklist for Slow Queries

| Check | Action |
| --- | --- |
| **Step 1** | Run `EXPLAIN ANALYZE`. |
| **Step 2** | Identify the step with the highest `actual time`. |
| **Step 3** | If it's a `Seq Scan`, add an index. |
| **Step 4** | If an index exists but isn't used, check for functions/type mismatches. |
| **Step 5** | Run `ANALYZE` to refresh statistics. |
| **Step 6** | Reduce the number of `JOINs` or rows fetched. |
